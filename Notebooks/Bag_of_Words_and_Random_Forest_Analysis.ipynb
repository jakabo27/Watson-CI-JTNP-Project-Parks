{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag of Words and Random Forest Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SvGAkhW0XjN",
        "colab_type": "text"
      },
      "source": [
        "Credit for most of this notebook goes to Chitipolu Sri Sudheera. See her version here: https://www.kaggle.com/srisudheera/nlp-bag-of-words-meets-bags-of-popcorn.  The model validation code at the end was developed separately.\n",
        "\n",
        "Google Colab notebook with more comments:  https://colab.research.google.com/drive/1IRVt-OXQs0deuCmU-95f3WFmvNHYp3EO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJetBHbbmMaS",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "875524f6-62e0-4878-80f2-f37a987c551e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "## Set up Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install a bunch of libraries\n",
        "!pip install -U -q PyDrive\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "print(\"\\nImported things sucessfully!\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\n",
            "Imported things sucessfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSUqc72ZIH7F",
        "colab_type": "text"
      },
      "source": [
        "# Getting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jn0PyCl_Cr-",
        "colab_type": "code",
        "outputId": "740d0a3b-8ff5-48b1-c034-ceb2d90a576f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "# load in the training data\n",
        "#data_file_path = \"/content/drive/My Drive/Documents/SCHOOL/Watson CI/Parks Project - Watson CI/Training Data/Labelbox Best 4.csv\"\n",
        "\n",
        "data_file_path = \"/content/drive/My Drive/Documents/SCHOOL/Watson CI/Parks Project - Watson CI/Training Data/Flipped Data.csv\"\n",
        "proanti_file   = \"/content/drive/My Drive/Documents/SCHOOL/Watson CI/Parks Project - Watson CI/Training Data/Non-Neutral ProAnti.csv\"\n",
        "sentiment_file = \"/content/drive/My Drive/Documents/SCHOOL/Watson CI/Parks Project - Watson CI/Training Data/Non-Neutral Sentiment.csv\"\n",
        "\n",
        "train=pd.read_csv(data_file_path, delimiter=',')\n",
        "train_sentiment = pd.read_csv(sentiment_file, delimiter=',')\n",
        "train_proanti = pd.read_csv(proanti_file, delimiter=',')\n",
        "\n",
        "#shuffle the data - optional but makes me feel better\n",
        "train = train.sample(frac=1).reset_index(drop=True)\n",
        "train_sentiment = train_sentiment.sample(frac=1).reset_index(drop=True)\n",
        "train_proanti   = train_proanti.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "#Define the column names in case you changed them later\n",
        "DataColName = \"Labeled Data\"  #The name of the column with the text data to analyze\n",
        "ExternalID = \"External ID\"\n",
        "# ClassifierColName = \"Sentiment\" #Sentiment or Pro/anti park #not really needed any more\n",
        "Sentiment = \"Sentiment\"\n",
        "ProAnti = \"Pro Anti Park\"\n",
        "\n",
        "# examine the loaded data\n",
        "print(train.shape)\n",
        "train[:10][:]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(288, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled Data</th>\n",
              "      <th>External ID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Pro Anti Park</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I had a really nice time in Joshua Tree this w...</td>\n",
              "      <td>/bignosebug/status/1120147383138967553</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sunrise in Joshua Tree\\n.\\nCan't wait to go ba...</td>\n",
              "      <td>/EMP_Creative/status/1071457436056346625</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Perfect Joshua Tree Day Trip: A One Day It...</td>\n",
              "      <td>/InAfricaNBeyond/status/1139961543557124096</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>So how did the latest government shutdown do t...</td>\n",
              "      <td>/swvacavalier/status/1140981246828392448</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Here are a few of the beauties I've spotted in...</td>\n",
              "      <td>/englishinmin/status/1111344469163438080</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>THE JOSHUA TREE CANNOT AFFECT MY LIFE SO.........</td>\n",
              "      <td>/lmbs83/status/1142489839272714240</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The inspiration for my first tattoo was the Jo...</td>\n",
              "      <td>/Marky_Boy1968/status/1177585075497635841</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>My mate @LewisHamilton-very happy tue see U ta...</td>\n",
              "      <td>/brent_otter7/status/1075146367234965504</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ready to have some fun in Joshua Tree</td>\n",
              "      <td>/CatherineViola/status/1111290392127823872</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Love this crew and their willingness to explor...</td>\n",
              "      <td>/BrentWalls39/status/1054229278592651265</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Labeled Data  ... Pro Anti Park\n",
              "0  I had a really nice time in Joshua Tree this w...  ...             0\n",
              "1  Sunrise in Joshua Tree\\n.\\nCan't wait to go ba...  ...             0\n",
              "2  The Perfect Joshua Tree Day Trip: A One Day It...  ...             0\n",
              "3  So how did the latest government shutdown do t...  ...             0\n",
              "4  Here are a few of the beauties I've spotted in...  ...             0\n",
              "5  THE JOSHUA TREE CANNOT AFFECT MY LIFE SO.........  ...             1\n",
              "6  The inspiration for my first tattoo was the Jo...  ...             0\n",
              "7  My mate @LewisHamilton-very happy tue see U ta...  ...             0\n",
              "8              Ready to have some fun in Joshua Tree  ...             0\n",
              "9  Love this crew and their willingness to explor...  ...             0\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iurD9VOHWsfL",
        "colab_type": "text"
      },
      "source": [
        "Clean the tweets into word lists\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTw4GCU9qSUn",
        "colab_type": "code",
        "outputId": "63a521ba-d3b4-4526-f6bd-9ae0c49126ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Clean the data\n",
        "words_to_keep = ['no', 'not', 'nor', 'against', 'why', 'dont', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "extra_words_to_remove = [\"gmail\", \"com\", \"or\", \"of\", \"I'll\"]  #Remove the words that we used as keywords for finding the tweets?\n",
        "\n",
        "mystopwords = [w for w in stopwords.words(\"english\") and extra_words_to_remove if not w in words_to_keep]\n",
        "\n",
        "#String that determines what characters we keep.  Currently getting rid of all numbers and non a-z characters except what you add in\n",
        "letters_only_sub_string = \"[^a-zA-Z?!:']\"\n",
        "\n",
        "# Define the function that cleans the tweet and makes it into lowercase words and joins it back together\n",
        "def review_to_words(original_tweet):\n",
        "    review_text=BeautifulSoup(original_tweet).get_text()\n",
        "    no_html = re.sub(r'http\\S+', '', review_text)             #Get rid of any http URLs\n",
        "    no_html = re.sub(r'pic.twitter.com\\S+', '', no_html)      #Get rid of any twitter picture urls\n",
        "    letters_only=re.sub(letters_only_sub_string,\" \",no_html)  #Get rid of numbers and special characters using the string key above\n",
        "\n",
        "    words=letters_only.lower().split()  #Convert to lowercase and split into individual words\n",
        "    meaningful_words=[w for w in words if not w in mystopwords]\n",
        "    return(' '.join(meaningful_words))\n",
        "\n",
        "#Test it on a few tweets\n",
        "example=BeautifulSoup(train[DataColName][0])\n",
        "print(\"Original:  \" + example.get_text())\n",
        "print(\"Cleaned:   \" + review_to_words(example.get_text()))\n",
        "example=BeautifulSoup(train[DataColName][1])\n",
        "print(\"Original:  \" + example.get_text())\n",
        "print(\"Cleaned:   \" + review_to_words(example.get_text()))\n",
        "example=BeautifulSoup(train[DataColName][2])\n",
        "print(\"Original:  \" + example.get_text())\n",
        "print(\"Cleaned:   \" + review_to_words(example.get_text()))\n",
        "\n",
        "# Clean our tweets - general\n",
        "num_tweets=train[DataColName].size\n",
        "clean_train_review=[]\n",
        "for i in range(0,num_tweets):\n",
        "    clean_train_review.append(review_to_words(train[DataColName][i]))\n",
        "print(\"Cleaned \" + str(num_tweets) + \" tweets and stored in clean_train_review\")\n",
        "\n",
        "#Clean our sentiment / proanti tweets\n",
        "clean_sentiment = []\n",
        "clean_proanti   = []\n",
        "for i in range(0,train_sentiment[DataColName].size):\n",
        "    clean_sentiment.append(review_to_words(train_sentiment[DataColName][i]))\n",
        "for i in range(0,train_proanti[DataColName].size):\n",
        "    clean_proanti.append(review_to_words(train_proanti[DataColName][i]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  so sad that i'm missing the snow in joshua tree\n",
            "Cleaned:   so sad that i'm missing the snow in joshua tree\n",
            "Original:  The Ukranian crochet museum at Joshua Tree is amazing! I recommend everyone check it out!\n",
            "Cleaned:   the ukranian crochet museum at joshua tree is amazing! i recommend everyone check it out!\n",
            "Original:  This shit makes me so mad. I can't read any more articles about the treatment of Joshua Tree during the government shut down or I'll implode with rage.\n",
            "Cleaned:   this shit makes me so mad i can't read any more articles about the treatment joshua tree during the government shut down i'll implode with rage\n",
            "Cleaned 288 tweets and stored in clean_train_review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DqL0R0PlFoTc"
      },
      "source": [
        "# Model optimizing and batch testing of variation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSRJW1Jb4FlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OPTIONAL\n",
        "# This section was for my testing to find the best compbination of parameters.  Please skip it for real stuff\n",
        "\n",
        "#Generate a set of x_data based on a variety of max_features\n",
        "max_features_list = [100, 250, 500, 750, 1000, 1500, 3000]\n",
        "X_data = []\n",
        "\n",
        "print(len(max_features_list))\n",
        "\n",
        "i = 0\n",
        "while i < len(max_features_list):\n",
        "  vectorizer=CountVectorizer(analyzer='word',tokenizer=None,preprocessor = None, stop_words = None, max_features = max_features_list[i])\n",
        "  train_data_features=vectorizer.fit_transform(clean_train_review)\n",
        "  train_data_features=train_data_features.toarray()\n",
        "  X_data.append(train_data_features)\n",
        "  i += 1\n",
        "\n",
        "# X_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vb3FCjnz-Eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OPTIONAL\n",
        "# WARNING This takes a long time.  Like 15 minutes per set of X_data\n",
        "# This section was for my testing to find the best compbination of parameters.  Please skip it for real stuff\n",
        "\n",
        "\n",
        "# Try to find the best parameters for our model\n",
        "# Source:  https://medium.com/@hjhuney/implementing-a-random-forest-classification-model-in-python-583891c99652\n",
        "# Also need to look at this:  https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
        "\n",
        "# WARNING This takes a long time.  Like 15 minutes per set of X_data\n",
        "\n",
        "X_final = train_data_features\n",
        "y = train[Sentiment]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] # number of trees in random forest (default num=10)\n",
        "max_features = ['auto', 'sqrt']  # number of features at every split\n",
        "\n",
        "# max depth\n",
        "max_depth = [int(x) for x in np.linspace(50, 500, num = 11)] \n",
        "max_depth.append(None)\n",
        "\n",
        "# create random grid\n",
        "random_grid = {\n",
        " 'n_estimators': n_estimators,\n",
        " 'max_features': max_features,\n",
        " 'max_depth': max_depth\n",
        " }\n",
        "# Random search of parameters\n",
        "rfc_random = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 100, cv = 3, verbose=10, random_state=42, n_jobs = -1)\n",
        "\n",
        "j = 0\n",
        "for this_X_Data in X_data:\n",
        "  # Fit the model\n",
        "  rfc_random.fit(this_X_Data, y)\n",
        "  # print results\n",
        "  print(\"For a max features of \" + str(max_features_list[j]))\n",
        "  print(rfc_random.best_params_)\n",
        "  #to-do - run/test the model with these params\n",
        "  j += 1\n",
        "\n",
        "\n",
        "## Jacob saving the results of running it\n",
        "# Vectorizer Max Features = 100 -->  'n_estimators': 200,  'max_features': 'sqrt', 'max_depth': 95\n",
        "# Vectorizer Max Features = 250 -->  'n_estimators': 1800, 'max_features': 'sqrt', 'max_depth': 95\n",
        "# Vectorizer Max Features = 500 -->  'n_estimators': 2000, 'max_features': 'auto', 'max_depth': 50\n",
        "# Vectorizer Max Features = 750 -->  'n_estimators': 200,  'max_features': 'sqrt', 'max_depth': 365\n",
        "# Vectorizer Max Features = 1000 --> 'n_estimators': 200,  'max_features': 'auto', 'max_depth': 320\n",
        "# Vectorizer Max Features = 1500 --> 'n_estimators': 1400, 'max_features': 'auto', 'max_depth': 50\n",
        "# Vectorizer Max Features = 3000 --> 'n_estimators': 800, 'max_features': 'sqrt', 'max_depth': 185\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YXeCegKJdn6",
        "colab_type": "text"
      },
      "source": [
        "# Main Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_9USFK84vxW",
        "colab_type": "code",
        "outputId": "4801e313-ca39-4260-9bcc-60294710380c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Setup\n",
        "vectorization_max_features = 1000\n",
        "Logistic_max_iterations = 500\n",
        "rf_estimators = 200\n",
        "rf_max_features = \"sqrt\"\n",
        "rf_max_depth = 100\n",
        "scoring_method = 'roc_auc' # from this list  https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "\n",
        "verbose_num = 0\n",
        "return_train = True\n",
        "k_folds = 5\n",
        "\n",
        "# Vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer=CountVectorizer(analyzer='word',tokenizer=None,preprocessor = None, stop_words = None, max_features = 500)\n",
        "train_data_features       = (vectorizer.fit_transform(clean_train_review)).toarray()\n",
        "train_sentiment_features  = (vectorizer.fit_transform(clean_sentiment)).toarray()\n",
        "train_proanti_features    = (vectorizer.fit_transform(clean_proanti)).toarray()\n",
        "\n",
        "# Vectorizer Max Features = 100 -->  'n_estimators': 200,  'max_features': 'sqrt', 'max_depth': 95\n",
        "# Vectorizer Max Features = 250 -->  'n_estimators': 1800, 'max_features': 'sqrt', 'max_depth': 95\n",
        "# Vectorizer Max Features = 500 -->  'n_estimators': 2000, 'max_features': 'auto', 'max_depth': 50\n",
        "# Vectorizer Max Features = 750 -->  'n_estimators': 200,  'max_features': 'sqrt', 'max_depth': 365\n",
        "# Vectorizer Max Features = 1000 --> 'n_estimators': 200,  'max_features': 'auto', 'max_depth': 320\n",
        "# Vectorizer Max Features = 1500 --> 'n_estimators': 1400, 'max_features': 'auto', 'max_depth': 50\n",
        "# Vectorizer Max Features = 3000 --> 'n_estimators': 800,  'max_features': 'sqrt', 'max_depth': 185\n",
        "\n",
        "\n",
        "#Set up our models for Random Forest and Logistic\n",
        "lin = LogisticRegression(solver='liblinear', max_iter=Logistic_max_iterations) # may need to tune C: inverse regularization strength\n",
        "rf = RandomForestClassifier(n_estimators=rf_estimators, max_features=rf_max_features, max_depth=rf_max_depth)\n",
        "X_final = train_data_features\n",
        "\n",
        "# Logistic Regression doc:    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "# Random Forest Classifier:   https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "# Word2vec notebook:          https://github.com/tthustla/twitter_sentiment_analysis_part11/blob/master/Capstone_part11.ipynb\n",
        "# Another word2vec Keras twitter sentiment analyzer:  https://www.ahmedbesbes.com/blog/sentiment-analysis-with-keras-and-word-2-vec\n",
        "\n",
        "\n",
        "### Model Validation\n",
        "# Define a confidence interval function\n",
        "from scipy.stats import t\n",
        "def conf_int(ar):\n",
        "  mean = np.mean(ar)\n",
        "  sem = np.std(ar) / np.sqrt(len(ar))\n",
        "  t_score = np.abs(t.ppf(0.05, len(ar)-1))\n",
        "  # print(\"T-Score:  \" + str(t_score))  #This value is just based on the size of the dataset so it will be the same for all models \n",
        "  return (mean - t_score * sem, mean + t_score * sem )\n",
        "\n",
        "### SENTIMENT\n",
        "### Train based on Sentiment (Positive/Negative)\n",
        "X_final = train_sentiment_features\n",
        "y = train_sentiment[Sentiment]\n",
        "\n",
        "#Linear Model\n",
        "cv_lin_sentiment = cross_validate(lin, X_final, y, cv=k_folds, scoring=scoring_method, return_train_score=return_train, verbose=verbose_num)\n",
        "#Random forest model\n",
        "cv_rf_sentiment = cross_validate(rf, X_final, y, cv=k_folds, scoring=scoring_method, return_train_score=return_train, verbose=verbose_num)\n",
        "#Get error of each model in dollars(?)\n",
        "print(\"Logistic Regression model Sentiment scores confidence interval: (%0.3f, %0.3f)\" % conf_int(cv_lin_sentiment['test_score']))\n",
        "print(\"Random Forest model Sentiment scores confidence interval: (%0.3f, %0.3f)\" % conf_int(cv_rf_sentiment['test_score']))\n",
        "\n",
        "sentiment_model_trained_rf = rf.fit(X_final, y)  #look into random forest \"out of bag error\" \n",
        "sentiment_model_trained_lin = lin.fit(X_final, y)\n",
        "sorted_word_idx_sentiment = np.argsort(sentiment_model_trained_lin.coef_)\n",
        "\n",
        "\n",
        "### Train a new model based on Pro/Anti Park\n",
        "X_final = train_proanti_features\n",
        "y = train_proanti[ProAnti]\n",
        "\n",
        "#Linear Model\n",
        "cv_lin_proanti = cross_validate(lin, X_final, y, cv=k_folds, scoring=scoring_method, return_train_score=return_train, verbose=verbose_num)\n",
        "#Random forest model\n",
        "cv_rf_proanti = cross_validate(rf, X_final, y, cv=k_folds, scoring=scoring_method, return_train_score=return_train, verbose=verbose_num)\n",
        "#Get error of each model in dollars(?)\n",
        "print(\"Logistic Regression model Pro/Anti scores confidence interval: (%0.3f, %0.3f)\" % conf_int(cv_lin_proanti['test_score']))\n",
        "print(\"Random Forest model Pro/Anti scores confidence interval: (%0.3f, %0.3f)\" % conf_int(cv_rf_proanti['test_score']))\n",
        "\n",
        "proanti_model_trained_rf = rf.fit(X_final, y)\n",
        "proanti_model_trained_lin = lin.fit(X_final, y)\n",
        "sorted_word_idx_proanti = np.argsort(proanti_model_trained_lin.coef_)\n",
        "\n",
        "\n",
        "# #temp - F1 \n",
        "# Logistic Regression model Sentiment scores confidence interval: (0.289, 0.519)\n",
        "# Random Forest model Sentiment scores confidence interval: (0.343, 0.515)\n",
        "# Logistic Regression model Pro/Anti scores confidence interval: (-0.026, 0.319)\n",
        "# Random Forest model Pro/Anti scores confidence interval: (-0.026, 0.319)\n",
        "\n",
        "\n",
        "#vs roc_auc:"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression model Sentiment scores confidence interval: (0.743, 0.833)\n",
            "Random Forest model Sentiment scores confidence interval: (0.703, 0.780)\n",
            "Logistic Regression model Pro/Anti scores confidence interval: (0.619, 0.780)\n",
            "Random Forest model Pro/Anti scores confidence interval: (0.568, 0.724)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTzoYM3mq1kV",
        "colab_type": "text"
      },
      "source": [
        "# Which words predict positive and negative sentiment?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eULFEf_rkqG",
        "colab_type": "code",
        "outputId": "af0f940a-d702-4b71-f7e1-74615bb28109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Only works if you use the linear / logistic model\n",
        "\n",
        "# Most negative words\n",
        "[vectorizer.get_feature_names()[ix] for ix in sorted_word_idx_sentiment[0,:30]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['road',\n",
              " 'because',\n",
              " 'river',\n",
              " 'whole',\n",
              " 'do',\n",
              " 'before',\n",
              " 'need',\n",
              " 'shut',\n",
              " 'grew',\n",
              " 'why',\n",
              " 'wtf',\n",
              " 'small',\n",
              " 'drive',\n",
              " 'many',\n",
              " 'canyon',\n",
              " 'your',\n",
              " 'damn',\n",
              " 'years',\n",
              " 'garden',\n",
              " 'being',\n",
              " 'travel',\n",
              " 'wildlife',\n",
              " 'been',\n",
              " 'palm',\n",
              " 'shutdown',\n",
              " 'art',\n",
              " 'world',\n",
              " 'sunrise',\n",
              " 'lol',\n",
              " 'outside']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDGPW9Zr_y0",
        "colab_type": "code",
        "outputId": "9c266f2c-6fd8-43d9-d838-63d1b86fa330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Most positive words\n",
        "[vectorizer.get_feature_names()[ix] for ix in sorted_word_idx_sentiment[0,-30:]][::-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ll',\n",
              " 'got',\n",
              " 'way',\n",
              " 'also',\n",
              " 'beautiful',\n",
              " 'summer',\n",
              " 'keep',\n",
              " 'ever',\n",
              " 'issues',\n",
              " 'perfect',\n",
              " 'galaxy',\n",
              " 'two',\n",
              " 'beauty',\n",
              " 'quiet',\n",
              " 'first',\n",
              " 'away',\n",
              " 'nationalparks',\n",
              " 'her',\n",
              " 'wanna',\n",
              " 'already',\n",
              " 'say',\n",
              " 'especially',\n",
              " 'air',\n",
              " 'visited',\n",
              " 'be',\n",
              " 'since',\n",
              " 'nationalpark',\n",
              " 'old',\n",
              " 'bliss',\n",
              " 'how']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ndchpOOnd2k",
        "colab_type": "text"
      },
      "source": [
        "Test the models on new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ROOU5HnsDis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load in the testing data\n",
        "data_file_path = \"/content/drive/My Drive/Documents/SCHOOL/Watson CI/Parks Project - Watson CI/Training Data/Best 4 Test Data.csv\"\n",
        "#Dumb bad test - test it on the training data!\n",
        "# data_file_path = \"/content/drive/My Drive/Documents/SCHOOL/Watson CI/Parks Project - Watson CI/Training Data/Labelbox Best 4.csv\"\n",
        "\n",
        "test = pd.read_csv(data_file_path, delimiter=',')\n",
        "\n",
        "#Define the column names in case you changed them later\n",
        "DataColName = \"Labeled Data\"  #The name of the column with the text data to analyze\n",
        "ExternalID  = \"External ID\"\n",
        "Sentiment   = \"Sentiment\"\n",
        "ProAnti     = \"Pro Anti Park\"\n",
        "\n",
        "# examine the loaded data\n",
        "test[:][:10]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqsgP99LntGO",
        "colab_type": "code",
        "outputId": "2dfa35b3-86f3-4512-eeae-50cbcea8ef6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        }
      },
      "source": [
        "# Create an empty list and append the clean reviews one by one\n",
        "num_tweets = len(test[ClassifierColName])\n",
        "clean_test_tweets= [] \n",
        "print(\"Cleaning and parsing the test set of tweets...\\n\")\n",
        "for i in range(0,num_tweets):\n",
        "    clean_tweet = review_to_words( test[DataColName][i] )\n",
        "    clean_test_tweets.append( clean_tweet )\n",
        "\n",
        "# Get a bag of words for the test set, and convert to a numpy array\n",
        "test_data_features = vectorizer.transform(clean_test_tweets)\n",
        "test_data_features = test_data_features.toarray()\n",
        "\n",
        "# Use the random forest to make sentiment label predictions\n",
        "sentiment_result_rf   = sentiment_model_trained_rf.predict(test_data_features)\n",
        "sentiment_result_lin  = sentiment_model_trained_lin.predict(test_data_features)\n",
        "proanti_result_rf     = proanti_model_trained_rf.predict(test_data_features)\n",
        "proanti_result_lin    = proanti_model_trained_lin.predict(test_data_features)\n",
        "\n",
        "# Copy the results to a pandas dataframe\n",
        "output = pd.DataFrame( data={DataColName:test[DataColName],  \n",
        "                            #  ExternalID:test[ExternalID], \n",
        "                             \"Sent RF\":sentiment_result_rf, \"Sent Lin:\":sentiment_result_lin,   \"Exp. Sent\":test[Sentiment],\n",
        "                             \"Pro/ant RF\":proanti_result_rf,  \"ProAnt Lin\":proanti_result_lin,    \"Exp. ProAnti\":test[ProAnti],\n",
        "                             } )\n",
        "\n",
        "#Optional - Write the result to a csv\n",
        "output_file = \"/content/drive/My Drive/Documents/SCHOOL/Watson CI/Parks Project - Watson CI/Training Data/Most Recent Saved Output.csv\"\n",
        "output.to_csv(output_file, index=False)\n",
        "\n",
        "#Display the results here\n",
        "output\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning and parsing the test set of tweets...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled Data</th>\n",
              "      <th>Sent RF</th>\n",
              "      <th>Sent Lin:</th>\n",
              "      <th>Exp. Sent</th>\n",
              "      <th>Pro/ant RF</th>\n",
              "      <th>ProAnt Lin</th>\n",
              "      <th>Exp. ProAnti</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"The Joshua Tree\"\\nHeaded up for one last go b...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Joshua Tree Weekend 2 &gt; Coachella Weekend 2 Ca...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bruh Joshua Tree Is Gonna Be Fkn Litty Bitchhh!!!</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I'm thrilled about the all NEW location for my...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KeeleyDonovan Many happy returns of the day -...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>It's #SundayMorning and I am sending my happy ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>A few shots from the wonderful experience of m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I always see cool people going to Joshua tree ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I was just at a wedding in Joshua Tree in Octo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Joshua Tree is losing it's Joshua trees! https...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>The night sky is beautiful. Go find your stars...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[OC] Just another beautiful day in Joshua Tree...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>You should try copper mountain first. It's in ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>It's the Joshua Tree's struggle that gives it ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>It's really bad in my part of L.A., and one of...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Made it home safe, put 2,500 miles on the rent...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Yay Palm Springs and Joshua Tree is so close! ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>We really said fuck it on Friday and left to J...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Shooting night skies in epic locations is one ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>The Gov't shutdown did 300 years of damage to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Thanks for paying attention to our park MIke w...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Labeled Data  ...  Exp. ProAnti\n",
              "0   \"The Joshua Tree\"\\nHeaded up for one last go b...  ...             1\n",
              "1   Joshua Tree Weekend 2 > Coachella Weekend 2 Ca...  ...             1\n",
              "2   Bruh Joshua Tree Is Gonna Be Fkn Litty Bitchhh!!!  ...             1\n",
              "3   I'm thrilled about the all NEW location for my...  ...             1\n",
              "4   @KeeleyDonovan Many happy returns of the day -...  ...             1\n",
              "5   It's #SundayMorning and I am sending my happy ...  ...             1\n",
              "6   A few shots from the wonderful experience of m...  ...             1\n",
              "7   I always see cool people going to Joshua tree ...  ...             0\n",
              "8   I was just at a wedding in Joshua Tree in Octo...  ...             1\n",
              "9   Joshua Tree is losing it's Joshua trees! https...  ...             1\n",
              "10  The night sky is beautiful. Go find your stars...  ...             1\n",
              "11  [OC] Just another beautiful day in Joshua Tree...  ...             1\n",
              "12  You should try copper mountain first. It's in ...  ...             1\n",
              "13  It's the Joshua Tree's struggle that gives it ...  ...             1\n",
              "14  It's really bad in my part of L.A., and one of...  ...             1\n",
              "15  Made it home safe, put 2,500 miles on the rent...  ...             1\n",
              "16  Yay Palm Springs and Joshua Tree is so close! ...  ...             1\n",
              "17  We really said fuck it on Friday and left to J...  ...             1\n",
              "18  Shooting night skies in epic locations is one ...  ...             1\n",
              "19  The Gov't shutdown did 300 years of damage to ...  ...             1\n",
              "20  Thanks for paying attention to our park MIke w...  ...             1\n",
              "\n",
              "[21 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}
